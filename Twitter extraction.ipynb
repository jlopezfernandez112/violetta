{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# My module\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiding secret API keys in Environment Variables\n",
    "consumer_key = config.CONSUMER_KEY\n",
    "consumer_secret = config.CONSUMER_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Relación tóxica OR Celos OR Chantaje OR Amenaza OR Controlador OR Violencia psicologica OR Infiel OR Gaslighting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access granted :)\n"
     ]
    }
   ],
   "source": [
    "# Check access to the API\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)\n",
    "if(api.verify_credentials):\n",
    "    print(\"Access granted :)\")\n",
    "else:\n",
    "    print(\"Access denied :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def connect_to_twitter_OAuth2(consumer_key=consumer_key, consumer_secret=consumer_secret):\n",
    "    \"\"\"Sets a connection to the twitter API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    consumer_key : set by default\n",
    "    consumer_secret : set by default\n",
    "    \"\"\"\n",
    "    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n",
    "\n",
    "def retrieve_tweets(api, since_id=None, max_id=None):\n",
    "    \"\"\"\n",
    "    It returns a twitter object with 100 tweets of a specific api response.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    api : api connection (required)\n",
    "    since_id : if given, it returns tweets with an ID greater than that (newer)\n",
    "    max_id : if given, it returns tweets with an ID less or equal than that (older) (max. 7 days prior)\n",
    "    \"\"\"\n",
    "    return api.search(q=query,\n",
    "                      lang='es',\n",
    "                      result_type='mixed',\n",
    "                      count=10,\n",
    "                      since_id=since_id,\n",
    "                      max_id=max_id,\n",
    "                      tweet_mode='extended')\n",
    "\n",
    "\n",
    "def extract_tweet_atributes(tweet_object):\n",
    "    \"\"\"It returns a Pandas DataFrame with a tweet per row and its attributes per column.\"\"\"\n",
    "    \n",
    "    tweets_list = []\n",
    "    \n",
    "    for tweet in tweet_object:\n",
    "        # Iterates over each tweet and gets its attributes\n",
    "        tweet_id = tweet.id   # Unique tweet identifier\n",
    "        text = tweet.full_text   # Sring, text of the tweet\n",
    "        screen_name = tweet.user.screen_name   # String, username\n",
    "        followers = tweet.user.followers_count   # Number of followers\n",
    "        retweet_count = tweet.retweet_count   # Number of retweets\n",
    "        favorite_count = tweet.favorite_count   # Number of favorites\n",
    "        created_at = tweet.created_at   # UTC time tweet created\n",
    "        source = tweet.source   # Utility used to post the tweet\n",
    "        location = tweet.user.location   # Location tweet was posted from\n",
    "        # Append attributes to list\n",
    "        tweets_list.append({'tweet_id':tweet_id,\n",
    "                            'text':text, \n",
    "                            'screen_name':screen_name,\n",
    "                            'followers':followers,\n",
    "                            'retweet_count':retweet_count, \n",
    "                            'favorite_count':favorite_count, \n",
    "                            'created_at':created_at, \n",
    "                            'source':source,\n",
    "                            'location': location})\n",
    "    # Creates a DataFrame\n",
    "    df = pd.DataFrame(tweets_list, columns=['tweet_id',\n",
    "                                            'text',\n",
    "                                            'screen_name',\n",
    "                                            'followers',\n",
    "                                            'retweet_count',\n",
    "                                            'favorite_count', \n",
    "                                            'created_at',\n",
    "                                            'source',\n",
    "                                            'location'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def first_cleaning(df):\n",
    "    \"\"\"It returns a DataFrame after dropping duplicates (subset=['tweet_id']) and sorting it (by='tweet_id')\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame to clean.\n",
    "    \"\"\"\n",
    "    df_no_dup = df.drop_duplicates(subset=['tweet_id'], ignore_index=True)\n",
    "    cleaned_df = df_no_dup.sort_values(by='tweet_id', ignore_index=True)\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "\n",
    "# Main functions\n",
    "def main_retrieval(file_path, last_id=None):\n",
    "    \"\"\"\n",
    "    Main retrieval function.\n",
    "    It makes 450 requests.\n",
    "    It saves a DataFrame to a csv in a given path.\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    + Last tweet id.\n",
    "    + DataFrame length\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : file where the DataFrame will be stored (append mode)\n",
    "    last_id : if given, it retrieves tweets only with a greter ID (older)\n",
    "    \"\"\"\n",
    "    # Set a connection to the api\n",
    "    api = connect_to_twitter_OAuth2()\n",
    "    # Set some required variables\n",
    "    number_of_requests = 450\n",
    "    dfs = []\n",
    "    # Main loop\n",
    "    for i in tqdm(range(number_of_requests)):\n",
    "        \n",
    "        violetta_tweets = retrieve_tweets(api, since_id=last_id)\n",
    "        df = extract_tweet_atributes(violetta_tweets)\n",
    "        # Set a new last_id. Next iteration starts taking tweets from it on\n",
    "        last_id = df['tweet_id'].max()\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df = first_cleaning(df)\n",
    "    last_id = df['tweet_id'].max()\n",
    "    # Saves df to a csv in the file_path, ignoring index, appending it, and not writting column names each time\n",
    "    df.to_csv(file_path, sep=',', index=False, mode='a', header=False)\n",
    "\n",
    "    return last_id, len(df)\n",
    "\n",
    "\n",
    "\n",
    "def long_term_retrieval(file_path, iterations=25, last_id=None):\n",
    "    \"\"\"\n",
    "    It aims to be retrieving tweets for a long period, 10 hours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : file where the DataFrame will be stored (append mode).\n",
    "    iterations : number of main_retrieval function calls. 15 iterations -> 11 hours period.\n",
    "    last_id : if given, it retrieves tweets only with a greter ID (older).\n",
    "    \"\"\"\n",
    "    lap = 0\n",
    "    while lap <= iterations:\n",
    "        # Try to retrieve tweets or sends an email if it cannot. It does not break the loop\n",
    "        try:\n",
    "            # Set the next last_id and the length of the DataFrame that just added to the csv\n",
    "            last_id, length = main_retrieval(file_path=file_path, last_id=last_id)\n",
    "            print(f'{length} new rows added to the csv.')\n",
    "        except:\n",
    "            print('Error!')\n",
    "            my_email.error_email()\n",
    "        # Release the counter and break the loop if necessary\n",
    "        lap += 1\n",
    "        if lap > iterations:\n",
    "            break\n",
    "        print(f'{(iterations + 1) - lap} laps to go.')  \n",
    "        # Time info\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(f'Getting some sleep @ {current_time}...')\n",
    "        # Getting some sleep til next main retrieval\n",
    "        time.sleep(16 * 60)\n",
    "        print('*' * 50)\n",
    "    print('Done :D\\nEnjoy it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Javi/Omdena/Violetta/violetta_tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [03:55<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 new rows added to the csv.\n",
      "25 laps to go.\n",
      "Getting some sleep @ 13:35:49...\n"
     ]
    }
   ],
   "source": [
    "long_term_retrieval(file_path, iterations=25, last_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
